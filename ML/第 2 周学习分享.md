时间：2019年4月1日 ~ 2019年4月7日

学习人 | 学习任务 | 学习心得 | 参考资料
-- | -- | -- | --
刘天宇 |	时间序列分析	| 今天主要学习了使用CNN来预测时间序列，重点明白了如何使用CNN对时间序列处理的方法
李小川 |	线性可分-逻辑回归	| （线性可分-逻辑回归）https://blog.csdn.net/lixc316/article/details/88970891
袁林	| 机器学习第三次作业代码重现| 	这周将第三次作业的python代码重现了一遍，np.array数组经过np.matrix转化成矩阵后，A*B即相当于线性代数矩阵的乘法（数组中的点乘（np.dot或者a@b））,scipy.optimize.minimize#最小化拟合函数,代价函数和梯度函数正则化时注意第一项不正则化，了解了numpy中matrix 和 array的区别。疑问点：梯度函数用for循环实现中for循环不太明白需要弄明白，代码能力需要加强。
潘瑞	| 《机器学习 周志华》第二章	| 本周学习模型评估与选择章节，内容还是比较简单，只涉及到一些基本概念和假设检验的知识。以前对于roc和auc略有生疏，有幸得到辅助南瓜书，加深了认识
陆开胜	| adaboost算法	| 这一周主要在写小论文，只学习了一个机器学习算法，求职招聘非常容易提问提升树算法，GBDT，xgboost，准备都要再好好看看，这周学习了adaboost算法。Adaboost思想是改变训练样本的权重，学习多个分类器，并将分类型进行线性组合成最终的强分类器，提高分类的效能。具体做法是1.每一轮改变训练样本的权重，提高被前一轮分类器错误分类的样本的权值，降低正确分类的样本的权重，得到下一轮的分类器；2.应用加权多数表决方法，加大分类误差率小的分类器的权重进行线性组合。算法步骤：1）初始化训练样本的权重分布（采用平均权值，保证可以学习最基本的分类器）2）使用具有权值分布的训练样本进行学习得到基本分类器3）计算学习到的分类器的分类误差率e4)计算分类器的系数α = 1/2*log[(1-e)/e]5）更新权值分布6)构建线性分类器 f(x) = αG(x)。Adaboost算法每次迭代可以减少其在训练数据集上的分类误差率，其每次训练误差有界限，且呈指数下降；Adaboost可以理解成模型是加法模型，损失函数是指数函数，学习算法是前向分布算法时的二分类学习算法，因只需要保证在每一步迭代中极小化损失函数。Python中的实现可以调用scikit-learn中Adaboost类库，AdaBoostClassifier使用了两种Adaboost分类算法的实现，SAMME和SAMME.R，AdaBoostClassifier调用需要调整参数弱分类器数量和学习率
肖宇| 	吴恩达、统计学习方法	| 1、这周，首先是重装了环境，感觉用anaconda装的坏境会正常一点；                                    2、学习了吴恩达的单变量线性回归的概念，以及通过python实现；                                         3、学习了李航的《统计学习方法》第一章和第二章，对两章里面的部分公式进行了简单的推导，以及使用python复现了第二章的感知机，以及通过sklearn实现感知机等；                                   4、总的来说，这周还是挺有收获的，但是感觉学习强度还是不够饱满。5、打卡让我知道了我一周干了啥，哈哈哈。不过，有没有讲机器学习中数学原理的书呢？向各位大佬请教一下
王汇源| 	吴恩达第一周作业，统计学习方法第一章| 	1. 上周把吴恩达第一周作业完成，但是还是有很多不明白，这周我又把作业重新看了敲了一遍，把每句话都好好理解了一下。基础差点，就慢点学，尽量搞明白2. 本周把统计学习方法第一章看完了，很多数学推导看不明白。第一章节看了三遍，还是有不理解的地方，想着先往后继续进行，在后面学习的过程中在继续理解前面有些不明白的地方。在学习的过程中是搭配黄博整理的代码一起学习的，感觉这样可以直接和代码结合，不会感觉那么无聊。本周因为有假期，回家休息了几天，下周继续加油，坚持每周都有学习有收获！！
晓林	| 吴恩达课程第二周+数据分析	| 1、python数据分析部分按照代码实现了很多案例，对知识有了较深刻的体会，下一步要加快进度2、吴恩达第二周课程看完，目前消化的感觉不太理想，按照黄博的笔记正在整理
刘嘉艺	| 搭建几个模型的运行环境，完成了模型的调优| 	搭建了骨架提取和风格迁移目标识别等模型的环境搭建，对自己建立的模型完成了调优，深刻体会到模型初值选择的重要性
王鸿博	| PCA的数学原理学习| 	上周复现论文遇到了许多问题，所以这周重新温习和学习了一些基础知识，映像最深刻的是张洋大佬写的PCA的数学原理，让我从新的角度认识了向量和矩阵的运算，以及PCA背后的数学原理。文章链接以前海龙大佬在知识星球发过。
冯雅欣| 	CTR算法模型	| 本周了解了一些广告预测算法CTR，并且学习FM的tensorflow代码。本地安装了XGboost框架。
成文浩| 	使用GNN的小样本学习	| Few-Shot Learning with Graph Neural Networks模型的输入是 s个有标签的x、r个无标签的x、t个待分类的x，输出是 t个待分类的x对应的标签。当r = 0, t = 1 and s = qK时，称为小样本学习。
